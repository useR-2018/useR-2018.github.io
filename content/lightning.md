+++
title = "Lightning Talk Schedule"
+++

The talks will take place on 11-13 July 2018 (click the interested talk for its abstract). Lightning talks will be 5 minutes, with room for discussion at the end of the session.  Information for presenters is [here](../presenter.html).

<!-- Tab links -->
<div class="tab">
  <button class="tablinks" onclick="openWday(event, 'Wednesday')" id="defaultOpen">Wednesday (July 11)</button>
  <button class="tablinks" onclick="openWday(event, 'Thursday')">Thursday (July 12)</button>
  <button class="tablinks" onclick="openWday(event, 'Friday')">Friday (July 13)</button>
</div>

<!-- Tab content -->
<div id="Wednesday" class="tabcontent">
  <table id="reg-sum">
    <col width="40"> <col width="40"> <col width="40"> <col width=40"> <col width="280"> <col width="40">
    <th>Time</th> <th>Session</th> <th>Presenter</th> <th>Venue</th> <th>Title</th> <th>Keywords</th>

    
    
    

  </table>
</div>

<div id="Thursday" class="tabcontent">
  <table id="reg-sum">
    <col width="40"> <col width="40"> <col width="40"> <col width=40"> <col width="280"> <col width="40">
    <th>Time</th> <th>Session</th> <th>Presenter</th> <th>Venue</th> <th>Title</th> <th>Keywords</th>

	
	
    <tr class="clickable" data-toggle="collapse" id="82" data-target=".82collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Tom Elliott</td>  <td>TBD</td>  <td>Historical data based priors for better bus arrival-time prediction</td>  <td>models, data mining, applications, space/time, big data</td></tr><tr class="collapse out budgets 82collapsed">  <td colspan="6">We have been developing a real-time bus arrival-time prediction framework, which so far relies solely on real-time data. However, we believe we can improve predictions (especially long-range, 20+ minutes) by incorporating historical data into the priors. This is especially useful in locations with infrequent buses, or before and after peak hour when travel times increase and decrease, respectively. Using a years' worth of GPS location data from buses in Auckland, New Zealand, we explore various models to develop time-dependent priors for bus travel time.</td></tr><tr class="clickable" data-toggle="collapse" id="92" data-target=".92collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Jono Tuke</td>  <td>TBD</td>  <td>Pachinko prediction</td>  <td>models, data mining</td></tr><tr class="collapse out budgets 92collapsed">  <td colspan="6">Social media is a great way for people to meet, chat and organise lunch, but it can also be used to meet, chat and organise protests. In this talk, I will explain how to use a Bayesian framework to predict social unrest from social media using Twitter as an example. So far so good, Bayesian modelling of Twitter data - but this is not that talk. How do you explain your modelling to the end-user? Not only that, but get them involved in the modelling?So let me show you how we used jam jars, coloured marbles, and the idea of Pachinko to explain our models to our collaborators, and how this started a conversation that lead to better models.</td></tr><tr class="clickable" data-toggle="collapse" id="146" data-target=".146collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Elvina Viennet</td>  <td>TBD</td>  <td>Uncertainty and sensitivity analyses: application to modelling the reproduction number of an infectious disease</td>  <td>models, performance</td></tr><tr class="collapse out budgets 146collapsed">  <td colspan="6">The usefulness of any model depends on the accuracy and reliability of its output. Uncertainty (UA) and sensitivity analyses (SA) are two approaches integral to the modelling process. UA enables to describe the range of possible outputs that derives from uncertainty in inputs, while SA enables a description of how sensitive the outcome variables are to inputs variation. These analyses allow for the identification of which parameters are important in explaining the outcome variable.Let’s consider a model for epidemic potential of Zika virus in Australia. To build this model we included input parameters that dictate the dynamics of disease transmission, leading to an output variable that describes how likely an outbreak can occur at time (t). We undertook the simple steps of i) UA, using a latin hypercube sampling method to generate 100 000 samples of the epidemic potential, and of ii) SA using the Partial Rank Correlation Coefficient analysis, to determine the statistical relationships between each input parameter and the epidemic potential while the other input parameters are kept constant.An overview of UA and SA in the context of infectious diseases modelling will be presented.</td></tr><tr class="clickable" data-toggle="collapse" id="176" data-target=".176collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Mark Wohlers</td>  <td>TBD</td>  <td>Deep Learning with Keras R to model Spectral Data.</td>  <td>algorithms, models</td></tr><tr class="collapse out budgets 176collapsed">  <td colspan="6">As deep learning has become more and more popular, TensorFlow has emerged as one of the dominant frameworks in the space. Keras, which runs on top of TensorFlow as well as other popular frameworks, is a high-level Python-based API with a user-friendly interface, large community, and clear documentation including an ever increasing number of examples.  Recently the Keras R interface became available, making training deep learning models in R very accessible by connecting to this large resource. This R package also makes installing dependencies relatively straightforward, although some manual installations are still required.We give an overview of our experiences in learning to train deep learning models in R, starting with more basic examples before moving to applying on our own Near-infrared spectroscopy (NIRS)-based dataset. This involved firstly training Convolutional Neural Networks, as suggested by Bjerrum et al. (2017) and then experimenting with other architectures such as LSTM networks.The implementation was carried out using Keras with the TensorFlow gpu backend while optimisation of the various hyperparameters used the RBayesianOptimization package.</td></tr><tr class="clickable" data-toggle="collapse" id="206" data-target=".206collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Saras Windecker</td>  <td>TBD</td>  <td>The zoon R package for reproducible and shareable species distribution modelling</td>  <td>models, reproducibility, community/education</td></tr><tr class="collapse out budgets 206collapsed">  <td colspan="6">The rapid growth of species distribution modelling (SDM) as an ecological discipline has resulted in a large and diverse set of methods and software for constructing and evaluating SDMs. The disjointed nature of the current SDM research environment hinders evaluation of new methods, synthesis of current knowledge and the dissemination of new methods to SDM users. The zoon r package aims to overcome these problems by providing a modular framework for constructing reproducible SDM workflows. Zoon modules are interoperable snippets of r code, each carrying a SDM method that zoon combines into a single analysis object. Rather than defining these modules, zoon draws modules from an open, version-controlled online repository. zoon makes it easy for SDM researchers to contribute modules to this repository, enabling others to rapidly deploy new methods in their own workflows or to compare alternative methods. Each workflow object created by zoon is a rerunnable record of the data, code and results of an entire SDM analysis. This can then be easily shared, scrutinised, reproduced and extended by the whole SDM research community.</td></tr><tr class="clickable" data-toggle="collapse" id="400" data-target=".400collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Stephanie Kobakian</td>  <td>TBD</td>  <td>Taipan: Images in action</td>  <td>image analysis</td></tr><tr class="collapse out budgets 400collapsed">  <td colspan="6">Image surveys made easy in R. This talk will demonstrate the use of the taipan R package. The package allows you to create a shiny app survey from a list of questions and a set of images. The app users will be able to answer questions regarding specific highlighted areas of the image and a range of question types are supported.</td></tr>
    
    

  </table>
</div>

<div id="Friday" class="tabcontent">
  <table id="reg-sum">
    <col width="40"> <col width="40"> <col width="40"> <col width=40"> <col width="280"> <col width="40">
    <th>Time</th> <th>Session</th> <th>Presenter</th> <th>Venue</th> <th>Title</th> <th>Keywords</th>

    
	<tr class="clickable" data-toggle="collapse" id="110" data-target=".110collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Yan Holtz</td>  <td>TBD</td>  <td>Getting rich quick with R & Cryptocurrencies?</td>  <td>applications, streaming data</td></tr><tr class="collapse out budgets 110collapsed">  <td colspan="6">Cryptocurrency has been a hot topic recently, with the Bitcoin price reaching 20k dollars in December 2017. I recently used R to recover the price of 5 cryptocurrencies for 2 months, on 5 platforms, at 10 second intervals. I then created a bot that automatically performed arbitrage: the simultaneous buying and selling of currency in different platforms in order to take advantage of differing prices.Both processes took advantage of the R potential for data visualization: the use of Dygraph (for interactive time series) and Flexdashboard (to create dashboard from R Markdown) enable the automatic generation of clean daily reports. The talk would first describe the technical process which allows the harvesting of data from both the public and private platform APIs. It would then describe how to visualize this information efficiently. It would also describe the price differences between platforms and will reveal whether arbitrage is a beneficial process for cryptocurrency.</td></tr><tr class="clickable" data-toggle="collapse" id="115" data-target=".115collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Ozan Cinar</td>  <td>TBD</td>  <td>poolR: Combining Dependent p-values</td>  <td>bioinformatics</td></tr><tr class="collapse out budgets 115collapsed">  <td colspan="6">Combining the p-values of a series of hypothesis tests is a task that is used in a variety of research fields. There are several well-known techniques for this purpose, such as Fisher’s and Stouffer’s methods. Furthermore, multiple testing methods, such as the Bonferroni correction can also be used for this purpose, by using the most significant corrected p-value from the set as the combined p-value. However, one important aspect of these methods (except for the Bonferroni correction) is that they assume independence among the tests and hence p-values to be combined. This assumption is known to be violated in some contexts. Therefore, the correlations among the p-values should be addressed appropriately. We present a new R package, poolR, that can be used for combining p-values. The package implements five methods for combining p-values. Furthermore, these methods can be modified in order to account for the dependence structure among the p-values. These modifications include adjustments based on the effective number of tests and the use of empirically-derived null distributions. Moreover, generalizations of Fisher’s and Stouffer’s methods for dependent tests are also implemented.</td></tr><tr class="clickable" data-toggle="collapse" id="131" data-target=".131collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Tamal Kumar De</td>  <td>TBD</td>  <td>Handling Missing Data in Single-Case Experiments</td>  <td>community/education, simulation, single-case, missing data</td></tr><tr class="collapse out budgets 131collapsed">  <td colspan="6">Single-case experiments have become increasingly popular in educational and behavioral research. However, analysis of single-case data is often complicated by missing or incomplete observations. This complication may lead to experiments no longer “meeting standards” set by, for example, the What Works Clearinghouse. Hence, it is important to determine optimal strategies for dealing with missing data.We conducted a simulation study in R to compare three randomization test strategies of handling missing data: (1) randomizing a missing data marker and calculating all reference statistics only for available data points; (2) estimating the missing data points by using minimum mean square error linear interpolation; and (3) multiple imputation methods based on resampling the available data points. We simulated datasets for phase designs, alternating treatments designs, and multiple baseline designs. Missingness was introduced in the simulated datasets by using multiple probabilities of data being “Missing Completely At Random”. The three strategies are compared in terms of Type I error rate and statistical power and are compared to the operating characteristics of the complete dataset.</td></tr><tr class="clickable" data-toggle="collapse" id="182" data-target=".182collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Dennis Wollersheim</td>  <td>TBD</td>  <td>What can you do with drugs?  Exploring Pharmaceutical Benefit Scheme prescription drug usage using R</td>  <td>visualisation, algorithms, models, databases, big data</td></tr><tr class="collapse out budgets 182collapsed">  <td colspan="6">In Australia, we have good prescription drug usage data, due to the Pharmaceutical Benefit Scheme, a population wide government insurance program. For example, the government recently released under creative commons license ten years of complete drug purchasing data for 10% of the population.  The data is also quite clean, having been processed through a government payment system, and therefore easy to use. R has a diverse, powerful and concise toolset, and is very useful to exploit this dataset.  It is a joy to be able to start with SQL queries on a postgres dataset,  pipe through various transformations, resulting in highly specific output forms, such as tables and graphs, maps and time series.  This workflow is replicable, archivable, expandable, and self documenting.   Remaining challenges mostly relate to dataset complexity, and domain specific problems, such as how to use the intermittent drug usage signal to isolate and characterise the different varieties of continuous drug usage, and more  generally, how to make use of wide heterogeneity of medical information. This talk will demonstrate what we have figured out in this area. </td></tr><tr class="clickable" data-toggle="collapse" id="190" data-target=".190collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Laurent Thibault</td>  <td>TBD</td>  <td>npbr: A Package for Nonparametric Boundary Regression in R</td>  <td>nonparametric statistics</td></tr><tr class="collapse out budgets 190collapsed">  <td colspan="6">The package npbr is the first free specialized software for data edge and frontier analysis in the statistical literature. It provides a variety of functions for the best known and most innovative approaches to nonparametric boundary estimation. The selected methods are concerned with empirical, smoothed, unrestricted as well as constrained fits under both single and multiple shape constraints. They also cover data envelopment techniques as well as robust approaches to outliers. The routines included in npbr are user friendly and afford a large degree of  flexibility in the estimation specifications. They provide smoothing parameter selection for the modern local linear and polynomial spline methods as well asfor some promising extreme value techniques. Also, they seamlessly allow for Monte Carlo comparisons among the implemented estimation procedures. This package will be very useful for statisticians and applied researchers interested in employing nonparametric boundary regression models. Its use is illustrated with a number of empirical applications and simulated examples.</td></tr><tr class="clickable" data-toggle="collapse" id="207" data-target=".207collapsed">  <td>14:00</td>  <td>Lightning talk</td>  <td>Robert King</td>  <td>TBD</td>  <td>R's early growth: easy dissemination of new stats</td>  <td>history</td></tr><tr class="collapse out budgets 207collapsed">  <td colspan="6">Why did R grow so rapidly in its early days? This is a reflection on the history of R, particularly on the reasons for its early growth.  R made it easy to code and distribute new statistical techniques.  To what extent does R retain this advantage today?</td></tr>
	

  </table>
</div>

<br>
